{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09acb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q \"google\"\n",
    "!pip install -U -q \"google.genai\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2542d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted data folder at: /workspaces/LLM-AI-AGENT-Demo/data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent 'data' folder to sys.path\n",
    "parent_data_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data'))\n",
    "if parent_data_path not in sys.path:\n",
    "    sys.path.append(parent_data_path)\n",
    "\n",
    "print(f\"Mounted data folder at: {parent_data_path}\")\n",
    "# Now you can import modules or access files from the 'data' folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d02cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q \"google\"\n",
    "!pip install -U -q \"google.genai\"\n",
    "!pip install -U -q \"chromadb\"\n",
    "!pip install -U -q \"python-dotenv\"\n",
    "!pip install -U -q \"langchain_community\"\n",
    "!pip install -U -q \"langchain_text_splitters\"\n",
    "!pip install -U -q \"pypdf\"\n",
    "!pip install -U -q \"google.generativeai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86240d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 783 documents from /workspaces/LLM-AI-AGENT-Demo/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:06<00:00, 12.1MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "\n",
    "# setting the environment\n",
    "\n",
    "DATA_PATH = parent_data_path\n",
    "CHROMA_PATH = r\"chroma_db\"\n",
    "\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"RAG\")\n",
    "\n",
    "# loading the document\n",
    "\n",
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "\n",
    "raw_documents = loader.load()\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(raw_documents)} documents from {DATA_PATH}\")\n",
    "\n",
    "# splitting the document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# preparing to be added in chromadb\n",
    "\n",
    "documents = []\n",
    "metadata = []\n",
    "ids = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for chunk in chunks:\n",
    "    documents.append(chunk.page_content)\n",
    "    ids.append(\"ID\"+str(i))\n",
    "    metadata.append(chunk.metadata)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# adding to chromadb\n",
    "\n",
    "\n",
    "collection.upsert(\n",
    "    documents=documents,\n",
    "    metadatas=metadata,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85903113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "Based on the provided text, here are some food options for breakfast, lunch, and dinner, keeping in mind that the data includes sample meal plans and mentions healthy eating habits:\n",
      "\n",
      "\n",
      "**Breakfast:**\n",
      "\n",
      "*   Cereals (50g)\n",
      "*   Milk (100ml)\n",
      "*   Pulses (20g)  \n",
      "*   Tea or Coffee (as per the sample plan)\n",
      "*   Idli (4 Nos), Dosa (3 Nos), Upma (1-1/2 Cup), Bread (4 slices), Porridge (2 Cups), or Corn flakes with milk (2 Cups)  (Options for non-vegetarians include substituting a pulse portion with egg/meat/chicken/fish.)\n",
      "\n",
      "\n",
      "**Lunch:**\n",
      "\n",
      "*   Cereals (100g)\n",
      "*   Rice (1 Cup)\n",
      "*   Pulses (20g)\n",
      "*   Dhal (1/2 Cup)\n",
      "*   Vegetables (150g total;  Veg. curry and salad)\n",
      "*   Milk or Curd (100ml or 1/2 cup)\n",
      "*   Pulkas (2 Nos)\n",
      "\n",
      "\n",
      "**Dinner:**\n",
      "\n",
      "*   Cereals (100g)\n",
      "*   Rice (1 Cup)\n",
      "*   Pulses (20g)\n",
      "*   Dhal (1/2 Cup)\n",
      "*   Phulkas (2 Nos)\n",
      "\n",
      "\n",
      "**Important Considerations from the text:**\n",
      "\n",
      "*   **Portion sizes:** Pay attention to the portion sizes specified.\n",
      "*   **Variety:**  The text emphasizes eating a variety of nutrient-rich foods. The meal plans provided offer examples of this, using different cereals, pulses, and vegetables.\n",
      "*   **Fat and Salt:**  Limit visible fat to 25g and salt to less than 5g per day during meal preparation.\n",
      "*   **Healthy Swaps:** The text suggests replacing dessert with fruit or yogurt.\n",
      "\n",
      "\n",
      "This is not an exhaustive list, and the provided data does not offer a wide range of options.  The sample meal plans focus on certain staples.  The best diet will vary based on individual needs and preferences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# setting the environment\n",
    "\n",
    "DATA_PATH = \"parent_data_path\"\n",
    "CHROMA_PATH = r\"chroma_db\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"RAG\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "\n",
    "user_query = input(\"What do you want to know about RAG Techniques?\\n\\n\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[user_query],\n",
    "    n_results=20\n",
    ")\n",
    "\n",
    "#print(results['documents'])\n",
    "#print(results['metadatas'])\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a health and wellness coach. You answer questions about Health and wellness.\n",
    "But you only answer based on knowledge I'm providing you. You don't use your internal\n",
    "knowledge and you don't make thins up.\n",
    "If you don't know the answer, just say: I don't know\n",
    "--------------------\n",
    "The data:\n",
    "\"\"\"+str(results['documents'])+\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "#print(system_prompt)\n",
    "\n",
    "response = model.generate_content(\n",
    "    contents = [\n",
    "        {\"role\":\"user\",\"parts\":[{\"text\":system_prompt}]},\n",
    "        {\"role\":\"model\",\"parts\":[{\"text\":\"Okay, I will answer questions about  Health and wellness  What is your question?\"}]},\n",
    "        {\"role\":\"user\",\"parts\":[{\"text\":user_query}]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\\n---------------------\\n\\n\")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
